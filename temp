class ConditionalRelationDetector(nn.Module):
    def __init__(self, visual_dims=(2048,1536,1024), text_dim=768, hidden_dim=512):
        super().__init__()
        self.num_scales = 3
        self.self_blocks = nn.ModuleList([SimpleSelfAttentionBlock(d, hidden_dim) for d in visual_dims])
        self.cross_blocks = nn.ModuleList([CrossModalAttentionBlock(hidden_dim, text_dim, hidden_dim) for _ in range(3)])
        self.class_heads = nn.ModuleList([nn.Linear(hidden_dim, 2) for _ in range(3)])
        self.loss_fn = nn.CrossEntropyLoss()

    def forward(self, visuals, text_features, text_mask=None, labels_crd=None):
        logits_list, probs_list, filtered = [], [], []

        for i in range(self.num_scales):
            HVi = visuals[i]
            HVi1 = self.self_blocks[i](HVi)
            HVi2 = self.cross_blocks[i](HVi1, text_features, text_mask)
            Hmax, _ = torch.max(HVi2, dim=1)
            logits = self.class_heads[i](Hmax)
            probs = F.softmax(logits, dim=-1)

            Gi = probs[:,1].unsqueeze(-1).unsqueeze(-1).expand_as(HVi2)
            HVi3 = Gi * HVi2

            logits_list.append(logits)
            probs_list.append(probs)
            filtered.append(HVi3)

        crd_logits = torch.stack(logits_list, dim=1)  # (B,3,2)
        crd_probs  = torch.stack(probs_list, dim=1)

        loss_crd = None
        if labels_crd is not None:
            loss_crd = self.loss_fn(crd_logits.view(-1,2), labels_crd.view(-1))

        return crd_logits, crd_probs, filtered, loss_crd

class VisualOpinionLearner(nn.Module):
    def __init__(self, hidden_dim=512, text_dim=768, backbone="resnet50"):
        super().__init__()
        self.backbone = backbone
        self.text_proj = nn.Linear(text_dim, hidden_dim) if text_dim != hidden_dim else nn.Identity()
        if backbone.startswith("resnet") or backbone.startswith("vit"):
            self.det_heads = nn.ModuleList([DetectHead(hidden_dim, hidden_dim) for _ in range(3)])
        else:
            self.det_heads = None

        self.W_proj = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(3)])
        self.WA = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(3)])
        self.gates = nn.ModuleList([nn.Linear(hidden_dim*2, 1) for _ in range(3)])
        self.attn_blocks = nn.ModuleList([CrossModalAttentionBlock(hidden_dim, hidden_dim, hidden_dim) for _ in range(3)])
        self.loss_fn = VOLLoss()

    def forward(self, visuals, text_features, text_mask=None, targets=None):
        HT = self.text_proj(text_features)
        H_hats, total_loss = [], None

        if targets is not None and self.det_heads is not None:
            total_loss = {"LLOC":0.0, "LCLS":0.0}
            for i,HVi in enumerate(visuals):
                preds = self.det_heads[i](self.W_proj[i](HVi))
                det_loss = self.loss_fn(preds, targets[i])
                total_loss["LLOC"] += det_loss["LLOC"]
                total_loss["LCLS"] += det_loss["LCLS"]

        for i,HVi in enumerate(visuals):
            Hv_proj = self.W_proj[i](HVi)
            Hv_att = self.attn_blocks[i](HVi, HT, text_mask)
            HvA_proj = self.WA[i](Hv_att)
            alpha = torch.sigmoid(self.gates[i](torch.cat([Hv_proj, HvA_proj], dim=-1)))
            H_hats.append(alpha*Hv_proj + (1-alpha)*HvA_proj)

        return H_hats, total_loss

# =============================== MSA =========================================
class MultiModalSentimentAnalyzer(nn.Module):
    def __init__(self, llm_model, hidden_dim=512):
        super().__init__()
        self.llm = llm_model
        self.d_model = llm_model.config.d_model
        self.vocab_size = llm_model.config.vocab_size
        self.WV2 = nn.Linear(196, 49)
        self.WV3 = nn.Linear(784, 49)
        self.fuse_proj = nn.Linear(hidden_dim*3, self.d_model)
        self.lm_head = nn.Linear(self.d_model, self.vocab_size)
        self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100)

    def forward(self, H_hats, E, text_mask, decoder_input_ids, labels=None):
        HV1, HV2, HV3 = H_hats
        HV2_49 = self.WV2(HV2.transpose(1,2)).transpose(1,2)
        HV3_49 = self.WV3(HV3.transpose(1,2)).transpose(1,2)
        H_cat = torch.cat([HV1, HV2_49, HV3_49], dim=-1)
        H_vis = self.fuse_proj(H_cat)

        # Debug prints (optional)
        # print("H_vis shape:", H_vis.shape)
        # print("E shape:", E.shape)
        # print("text_mask shape:", text_mask.shape)

        # Use only text embeddings and mask for BART encoder
        enc_in = E
        enc_mask = text_mask

        enc_out = self.llm.model.encoder(inputs_embeds=enc_in,
                                         attention_mask=enc_mask,
                                         return_dict=True)
        # print("encoder_hidden_state shape:", enc_out.last_hidden_state.shape)
        # print("decoder_input_ids shape:", decoder_input_ids.shape)
        # print("labels shape:", labels.shape if labels is not None else None)

        dec_out = self.llm.model.decoder(input_ids=decoder_input_ids,
                                         encoder_hidden_states=enc_out.last_hidden_state,
                                         encoder_attention_mask=enc_mask,
                                         return_dict=True)

        # FIX: Compute logits from decoder output
        logits = self.lm_head(dec_out.last_hidden_state)

        loss = None
        if labels is not None:
            loss = self.loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))

        return logits, loss